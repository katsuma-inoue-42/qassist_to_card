{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# QAssist subprint to card\n",
    "\n",
    "(last update: May 5, 2024)\n",
    "\n",
    "QAssistのサブプリントを分割してデッキ化するソフトウェアです。\n",
    "Ankiでの使用を想定し、デッキをロードするためのCSVファイル (表ファイル)も同時に生成します。\n",
    "家庭内で依頼されて作成したものを、個人学習の効率化の目的の下一般公開しています。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ライセンス・免責事項\n",
    "\n",
    "**MIT License**で公開します。\n",
    "つまり著作権および許諾表示により、自由な使用、改変、複製、再頒布が可能です。\n",
    "本ソフトウェアの使用または第三者への提供によって生じるいかなる損害や結果に対して、開発者は一切責任を負いません。\n",
    "\n",
    "また本ソフトウェアは個人利用の範疇での使用を想定しております。\n",
    "**生成されたカードデッキの著作権は使用した資料の著作者に属する**点に留意し、その公開や共有は原則行わないでください。\n",
    "\n",
    "加えて公開時点での資料を対象に最適化されており、将来の改訂により適切に動作しなくなる可能性があります。\n",
    "その場合は、各自内部パラメータやコードの修正が必要になりますが予めご了承ください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title MIT License\n",
    "#\n",
    "# Copyright (c) 2024 Katsuma Inoue\n",
    "#\n",
    "# Permission is hereby granted, free of charge, to any person obtaining a\n",
    "# copy of this software and associated documentation files (the \"Software\"),\n",
    "# to deal in the Software without restriction, including without limitation\n",
    "# the rights to use, copy, modify, merge, publish, distribute, sublicense,\n",
    "# and/or sell copies of the Software, and to permit persons to whom the\n",
    "# Software is furnished to do so, subject to the following conditions:\n",
    "#\n",
    "# The above copyright notice and this permission notice shall be included in\n",
    "# all copies or substantial portions of the Software.\n",
    "#\n",
    "# THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n",
    "# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n",
    "# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL\n",
    "# THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n",
    "# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING\n",
    "# FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER\n",
    "# DEALINGS IN THE SOFTWARE."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 使い方\n",
    "\n",
    "Google DriveとGoogle Colaboratoryを使用したオンラインでの使用を想定しています。\n",
    "また自前でPythonの環境を構築できる場合は、ローカル (手元の環境)でも動作できます。\n",
    "ここでは前者のオンラインの方法を説明します (後者の方法でも認証プロセスがないだけで大差はないです)。\n",
    "\n",
    "要約すると以下の4ステップにより完了します。\n",
    "なお不明な点はGitHubのissue欄からお問い合わせください (多忙のため返信に時間を要する場合がありますが予めご了承ください)。  \n",
    "\n",
    "\n",
    "1. ファイルのアップロード\n",
    "2. `pdf_folder`、`blank_suffix`、`filled_suffix`を指定\n",
    "3. `ctrl+f9`により実行\n",
    "4. Zipのダウンロードとフォルダの展開 (collection.media以下)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. ファイルの命名・アップロード\n",
    "まず空欄と回答のpdfファイルの接頭辞(先頭のファイル名)と接尾辞 (後ろのファイル名) を**一貫性のある形**で揃えてください。\n",
    "例えば単元`A`・`B`・`C`に関して空欄と回答ファイルを以下のように命名してください。\n",
    "\n",
    "```\n",
    "A_blank.pdf, A_filled.pdf  #単元Aに関する空欄 & 回答ファイル\n",
    "B_blank.pdf, B_filled.pdf  #単元Bに関する空欄 & 回答ファイル\n",
    "C_blank.pdf, C_filled.pdf  #単元Cに関する空欄 & 回答ファイル\n",
    "```\n",
    "予め揃っている場合は特に変更は必要ないです。\n",
    "ファイル名の準備ができたら適当なフォルダをGoogle Drive上に作成してアップロードしてください。\n",
    "\n",
    "<details><summary>※多少知識のある方向け</summary>\n",
    "\n",
    "あるいは揃えなくても、セル内にある変数`file_name_tuples`に直接ファイル名を記述しても実行できます。\n",
    "コメントアウトされた例に習い、空欄ファイル、回答ファイルの順で指定された`tuple`の`list`を指定して下さい。\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. フォルダ名・解像度の指定\n",
    "\n",
    "次に「各種設定」以下のセル (四角で囲われた領域)で各種パラメータを指定してください。\n",
    "以下各パラメータに関する説明を記載します。\n",
    "\n",
    "- `pdf_folder`: 前ステップでpdfをアップロードしたフォルダ名。デフォルトではホームディレクトリ以下`./data`を指定。\n",
    "- `output_folder`: 生成されたデッキが保存されるフォルダ名。デフォルトではホームディレクトリ以下`./output`を指定。\n",
    "- `output_name`: 生成されるzipファイルの名前。デフォルトでは`qassist`を指定。\n",
    "- `blank_suffix`: 前ステップで設定した空欄ファイルの接尾辞。デフォルトでは`_blank.pdf`を指定。\n",
    "- `filled_suffix`: 前ステップで設定した回答ファイルの接尾辞。デフォルトでは`_filled.pdf`を指定。\n",
    "- `dpi`: 生成されるカードの画面解像度 (dots per inch)。デフォルトでは`200`を指定 (※この数値のおおよそ2乗に最終的なファイルサイズが比例する点に注意)。\n",
    "- `output_separated_csv`: 各単元に関してCSVファイルを別々に出力。デフォルトでは`True`。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. 認証と実行\n",
    "\n",
    "前ステップでパラメータを指定したら、`ctrl + F9`を押すか、上から順にセルを実行 (`ctrl + Enter`か左側のボタンをクリック)して実行してください。\n",
    "途中Google Driveの認証画面が出ますが、すべてアクセスを許可してください。\n",
    "\n",
    "1ファイルあたり10~30秒程度で処理が完了し、最終的に`{output_folder}`以下に`{output_name}.zip`が生成されます (デフォルトでは`./output/qassist.zip`という名前のファイルが生成されると思います)。\n",
    "\n",
    "途中でエラーが発生した場合は、再度設定を見直し、最初から実行し直してください (同じく`ctrl + F9`が便利です)。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. 展開と配置\n",
    "生成されたzipファイル (以下`qassist.zip`) をAnkiが指定するフォルダに展開してください。\n",
    "\n",
    "Windowsの場合は\n",
    "```\n",
    "C:\\Users\\{ユーザー名}\\AppData\\Roaming\\Anki2\\{Ankiのユーザー名}\\collection.media\n",
    "```\n",
    "OS Xの場合は\n",
    "```\n",
    "~/Library/Application Support/Anki2/{Ankiのユーザー名}/Collection.media \n",
    "```\n",
    "となると思います ([公式ドキュメント](https://docs.ankiweb.net/files.html)参照)。\n",
    "\n",
    "その後「ファイルをインポート | Import File」から`info.csv`をロードしてください。\n",
    "`info.csv`は5列からなる表データで、それぞれの列は以下のデータが保持されています。\n",
    "\n",
    "- 1列目: 空欄カードのパス (とその[HTML](https://docs.ankiweb.net/importing/text-files.html#importing-media))\n",
    "- 2列目: 回答カードのパス (とその[HTML](https://docs.ankiweb.net/importing/text-files.html#importing-media))\n",
    "- 3列目: 章の名前\n",
    "- 4列目: 章・節・項の名前 (空白区切り)\n",
    "- 5列目: 単元 + ページ数 (例: `A_6p`は単元Aの6ページ目を表す)\n",
    "\n",
    "表面と裏面はそれぞれ1列目と2列目を指定してください。\n",
    "3列目以降のいずれかをタグとして指定できます。\n",
    "デフォルトでは3列目ですが、より詳細に4列目の指定も可能です。\n",
    "より学習単元を限定して学習したい場合にご利用してください。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 各種設定\n",
    "\n",
    "はじめに前節の「使い方」をお読みになってください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title フォルダ名・解像度の設定\n",
    "\n",
    "pdf_folder = './data'  #@param {type:'string'}\n",
    "output_folder = './output'  #@param {type:'string'}\n",
    "output_name = 'qassist'  #@param {type:'string'}\n",
    "blank_suffix = '_blank.pdf'  #@param {type:'string'}\n",
    "filled_suffix = '_filled.pdf'  #@param {type:'string'}\n",
    "dpi = 200  #@param {type:'number'}\n",
    "output_separated_csv = True  #@param {type:\"boolean\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Google Driveの認証・ライブラリのインストール・ファイルの読み込み\n",
    "import sys\n",
    "import glob\n",
    "from collections import Counter\n",
    "\n",
    "if 'google.colab' in sys.modules:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/gdrive')\n",
    "    %cd /content/gdrive/My Drive/\n",
    "    %pip install PyMuPDF\n",
    "\n",
    "prefixes = []\n",
    "file_name_tuples = []\n",
    "pdf_files = glob.glob(f'{pdf_folder}/*.pdf')\n",
    "for file_name in pdf_files:\n",
    "    if file_name.endswith(filled_suffix):\n",
    "        prefixes.append(file_name.removesuffix(filled_suffix))\n",
    "    elif file_name.endswith(blank_suffix):\n",
    "        prefixes.append(file_name.removesuffix(blank_suffix))\n",
    "for prefix, count in Counter(prefixes).items():\n",
    "    if count == 2:\n",
    "        file_name_tuples.append([\n",
    "            f\"{prefix}{blank_suffix}\",\n",
    "            f\"{prefix}{filled_suffix}\",\n",
    "        ])\n",
    "        print('{} & {}'.format(*file_name_tuples[-1]))\n",
    "\n",
    "\n",
    "# ファイル名を指定する場合はここをコメントアウトして記述してください (カンマ区切りに注意)\n",
    "# file_name_tuples = [\n",
    "#     ['a_blank.pdf', 'a_filled.pdf'],\n",
    "#     ['b_blank.pdf', 'b_filled.pdf'],\n",
    "# ]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. パラメータの指定・関数の実装\n",
    "※ 具体的な処理を実装しています。修正に興味のある方は展開してみてください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title 領域の境界 (値の変更は非推奨)\n",
    "\n",
    "header_threshold = 40  #@param {type:\"slider\", min:0, max:780, step:1}\n",
    "footer_threshold = 750  #@param {type:\"slider\", min:0, max:780, step:1} \n",
    "left_indent_threshold = 44  #@param {type:\"slider\", min:0, max:540, step:1} \n",
    "side_note_threshold = 395  #@param {type:\"slider\", min:0, max:540, step:1} \n",
    "\n",
    "layout_info = dict(\n",
    "    header_threshold=header_threshold,\n",
    "    footer_threshold=footer_threshold,\n",
    "    left_indent_threshold=left_indent_threshold,\n",
    "    side_note_threshold=side_note_threshold,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title 関数の定義\n",
    "\n",
    "import os\n",
    "import re\n",
    "import sys\n",
    "import fitz\n",
    "import unicodedata\n",
    "import pandas as pd\n",
    "\n",
    "from collections import defaultdict\n",
    "from tqdm.notebook import trange, tqdm\n",
    "\n",
    "\n",
    "def concat_lines(lines):\n",
    "    text = ''\n",
    "    for line in lines:\n",
    "        for span in line['spans']:\n",
    "            text += span['text']\n",
    "    return text\n",
    "\n",
    "\n",
    "def extract_bbox(\n",
    "        doc, page_number, last_problem=None,\n",
    "        header_threshold=40, footer_threshold=750,\n",
    "        left_indent_threshold=44, side_note_threshold=395, verbose=False):\n",
    "    problems = []\n",
    "    page = doc[page_number - 1]\n",
    "    texts = page.get_text('dict')\n",
    "    blocks = texts['blocks']\n",
    "    page_w, page_h = texts['width'], texts['height']\n",
    "    problem_id = 0\n",
    "    if last_problem is None:\n",
    "        lecture_id = ''\n",
    "        chapter_id, chapter_name = 0, ''\n",
    "        section_id, section_name = 0, ''\n",
    "        subsec_name = ''\n",
    "    else:\n",
    "        lecture_id = last_problem['lecture_id']\n",
    "        chapter_id, chapter_name = last_problem['chapter_id'], last_problem['chapter_name']\n",
    "        section_id, section_name = last_problem['section_id'], last_problem['section_name']\n",
    "        subsec_name = last_problem['subsec_name']\n",
    "    for block in blocks:\n",
    "        if 'lines' in block:\n",
    "            block['text'] = concat_lines(block['lines']).strip()\n",
    "        else:\n",
    "            block['text'] = ''\n",
    "    blocks = filter(lambda v: v['text'] != '', blocks)  # remove empty block\n",
    "    images = []\n",
    "    for image in page.get_image_info():\n",
    "        bbox = image['bbox']\n",
    "        if bbox[0] >= left_indent_threshold:\n",
    "            image['text'] = ''\n",
    "            images.append(image)\n",
    "    drawings = []\n",
    "    for info in page.get_drawings():\n",
    "        bbox = tuple(info['rect'])\n",
    "        if bbox[0] >= left_indent_threshold:\n",
    "            drawings.append(dict(bbox=bbox, text=''))\n",
    "    elements = list(blocks) + list(images) + list(drawings)\n",
    "    elements = filter(lambda v: v['bbox'][0] <= side_note_threshold, elements)  # remove side notes (x0)\n",
    "    elements = filter(lambda v: v['bbox'][1] <= footer_threshold, elements)  # remove footer (y0)\n",
    "    elements = sorted(elements, key=lambda v: v['bbox'][1])  # sorted (y0)\n",
    "    for idx, element in enumerate(elements):\n",
    "        bbox = element['bbox']\n",
    "        text = element['text']\n",
    "        x0, y0, x1, y1 = bbox\n",
    "        is_new_section, is_main_text = False, True\n",
    "        if y0 <= header_threshold:\n",
    "            # extract info from the header\n",
    "            if match := re.match(r'^([A-Z|a-z|Ａ-Ｚ|ａ-ｚ]+)(\\d+|[０-９]+)(.+)【.+】', text):\n",
    "                # extract chapters (e.g., A01 ...【...】)\n",
    "                lecture_id = unicodedata.normalize('NFKC', match[1])\n",
    "                chapter_id = int(match[2])\n",
    "                chapter_name = match[3]\n",
    "            is_new_section = False\n",
    "            is_main_text = False\n",
    "        elif x0 < left_indent_threshold:\n",
    "            # extract info from the main text\n",
    "            if (match := re.match(r'^(\\d+|[０-９]+)\\.(.+)$', text)):\n",
    "                # extract sections (e.g., 1. ...)\n",
    "                section_id = int(match[1])\n",
    "                section_name = match[2]\n",
    "                subsec_name = ''\n",
    "                problem_id = 0\n",
    "                is_new_section = True\n",
    "                is_main_text = False\n",
    "            elif (match := re.match(r'^【(.+)】', text)):\n",
    "                # extract subsections (e.g., 【...】)\n",
    "                subsec_name = match[1]\n",
    "                is_new_section = False\n",
    "                is_main_text = False\n",
    "            elif match := re.match(r'^□([①-⑳|㉑-㊿])(.+)$', text):\n",
    "                # extract problems (e.g., □① ...)\n",
    "                problem_id = int(unicodedata.normalize('NFKC', match[1]))\n",
    "                is_new_section = True\n",
    "                is_main_text = True\n",
    "        if is_new_section:\n",
    "            problems.append(dict(\n",
    "                bboxes = [], page_number=page_number, lecture_id=lecture_id,\n",
    "                chapter_id=chapter_id, chapter_name=chapter_name,\n",
    "                section_id=section_id, section_name=section_name,\n",
    "                subsec_name=subsec_name, problem_id=problem_id, text_length=0))\n",
    "        if is_main_text and len(problems) > 0:\n",
    "            problems[-1]['bboxes'].append(bbox)\n",
    "            problems[-1]['text_length'] += len(text)\n",
    "        if verbose:\n",
    "            bbox_str = ','.join(map('{:3.0f}'.format, bbox))\n",
    "            print('{} p.{:<3} {:>2} {:>2} ({:>3},{:>3},{:>3}): {}'.format(\n",
    "                bbox_str, page_number, is_main_text, lecture_id,\n",
    "                chapter_id, section_id, problem_id, text))\n",
    "    problems = filter(lambda v: len(v['bboxes']) > 0, problems)\n",
    "    problems = filter(lambda v: v['text_length'] > 0, problems)\n",
    "    problems = list(problems)\n",
    "    y0_acc = [problem['bboxes'][0][1] for problem in problems]\n",
    "    y0_acc.append(footer_threshold)\n",
    "    for problem_id, (y0, y1) in enumerate(zip(y0_acc[:-1], y0_acc[1:])):\n",
    "        tables = page.find_tables(clip=(0, y0, int(page_w), y1))\n",
    "        for idx, table in enumerate(tables.tables):\n",
    "            if table.bbox[0] <= side_note_threshold and table.bbox[1] <= footer_threshold:\n",
    "                problems[problem_id]['bboxes'].append(table.bbox)\n",
    "    for problem in problems:\n",
    "        bboxes = problem['bboxes']\n",
    "        x0 = min(map(lambda t: t[0], bboxes))\n",
    "        y0 = min(map(lambda t: t[1], bboxes))\n",
    "        x1 = max(map(lambda t: t[2], bboxes))\n",
    "        y1 = max(map(lambda t: t[3], bboxes))\n",
    "        problem['bbox'] = (x0, y0, x1, y1)\n",
    "        problem['within_main_view'] = (\n",
    "            y0 >= header_threshold) and (\n",
    "                y1 <= footer_threshold) and (\n",
    "                    x1 <= side_note_threshold)\n",
    "    return problems\n",
    "\n",
    "\n",
    "def render_problems(\n",
    "        problems, doc_dict, dpi=200,\n",
    "        save_dir='./out', image_dir='qassist', \n",
    "        add_prefix=True, dry_run=False):\n",
    "    pbar = tqdm(problems)\n",
    "    for idx, problem in enumerate(pbar):\n",
    "        lecture_id = problem['lecture_id']\n",
    "        chapter_id = problem['chapter_id']\n",
    "        section_id = problem['section_id']\n",
    "        problem_id = problem['problem_id']\n",
    "        for doc_type_name, doc in doc_dict.items():\n",
    "            page = doc[problem['page_number'] - 1]\n",
    "            tar_pix = page.get_pixmap(clip=problem['bbox'], dpi=dpi)\n",
    "            if not problem['within_main_view']:\n",
    "                tar_pix.clear_with(255)\n",
    "                for bbox in problem['bboxes']:\n",
    "                    src_pix = page.get_pixmap(clip=bbox, dpi=dpi)\n",
    "                    tar_pix.copy(src_pix, src_pix.irect)\n",
    "            file_name = '{:02d}_{:02d}_{:02d}_{}'.format(\n",
    "                chapter_id, section_id, problem_id, doc_type_name)\n",
    "            if add_prefix:\n",
    "                file_name = '{:04d}_{}'.format(idx + 1, file_name)\n",
    "            file_name = f'{image_dir}/{lecture_id}/{file_name}'\n",
    "            problem[f'{doc_type_name}_file'] = file_name\n",
    "            if dry_run:\n",
    "                print(file_name)\n",
    "            else:\n",
    "                path = f'{save_dir}/{file_name}.png'\n",
    "                os.makedirs(os.path.dirname(path), exist_ok=True)\n",
    "                # if os.path.exists(path):\n",
    "                #     print('Warning: {} is duplicate'.format(path))\n",
    "                tar_pix.save(path)\n",
    "\n",
    "\n",
    "def create_accumulated_csv(problems, delimiter=' '):\n",
    "    lecture_id = ''\n",
    "    rows = []\n",
    "    for problem in problems:\n",
    "        blank_name = '<img src=\"{}.png\" />'.format(problem['blank_file'])\n",
    "        filled_name = '<img src=\"{}.png\" />'.format(problem['filled_file'])\n",
    "        chapter_name = problem['chapter_name']\n",
    "        section_name = problem['section_name']\n",
    "        subsec_name = problem['subsec_name']\n",
    "        page_info = '{}_p{}'.format(problem['lecture_id'], problem['page_number'])\n",
    "        tags = delimiter.join([chapter_name, section_name, subsec_name])\n",
    "        rows.append([blank_name, filled_name, chapter_name, tags, page_info])\n",
    "        lecture_id = problem['lecture_id']\n",
    "    df = pd.DataFrame(rows, columns=['blank', 'filled', 'chapter', 'tags', 'page'])\n",
    "    return lecture_id, df\n",
    "\n",
    "\n",
    "def run_all(\n",
    "        file_name_tuples, save_dir='./output',\n",
    "        image_dir='qassist', dpi=200,\n",
    "        output_separated_csv=True,\n",
    "        header_threshold=40, footer_threshold=750,\n",
    "        left_indent_threshold=44, side_note_threshold=395):\n",
    "    dfs = []\n",
    "    for blank_file, filled_file in file_name_tuples:\n",
    "        if not os.path.exists(blank_file):\n",
    "            print(f'Warning: {blank_file}! does not exist!')\n",
    "            continue\n",
    "        if not os.path.exists(filled_file):\n",
    "            print(f'Warning: {filled_file}! does not exist!')\n",
    "            continue\n",
    "        doc_fld = fitz.open(filled_file)\n",
    "        doc_blk = fitz.open(blank_file)\n",
    "        if len(doc_blk) != len(doc_blk):\n",
    "            print('Warning: the numbers of pages do not match!')\n",
    "            continue\n",
    "        print(f'Successfully loaded from {blank_file} & {filled_file}')\n",
    "        problems = []\n",
    "        last_problem = None\n",
    "        # for page_number in trange(3, 4):\n",
    "        pbar = trange(1, len(doc_fld) + 1)\n",
    "        for page_number in pbar:\n",
    "            out = extract_bbox(\n",
    "                doc_fld, page_number,\n",
    "                last_problem=last_problem,\n",
    "                header_threshold=header_threshold,\n",
    "                footer_threshold=footer_threshold,\n",
    "                left_indent_threshold=left_indent_threshold,\n",
    "                side_note_threshold=side_note_threshold,\n",
    "                verbose=False)\n",
    "            problems += out\n",
    "            if len(problems) > 0:\n",
    "                last_problem = problems[-1]\n",
    "        print('Now rendering ...')\n",
    "        render_problems(\n",
    "            problems, dict(filled=doc_fld, blank=doc_blk),\n",
    "            dpi=dpi, save_dir=save_dir, image_dir=image_dir, dry_run=False)\n",
    "        lecture_id, df = create_accumulated_csv(problems)\n",
    "        if output_separated_csv:\n",
    "            df.to_csv(f'{save_dir}/{image_dir}/info_{lecture_id}.csv', header=False, index=False)\n",
    "        dfs.append([lecture_id, df])\n",
    "    if len(dfs) == 0:\n",
    "        print('Warning: no file is processed!')\n",
    "        return\n",
    "    dfs = pd.concat([df for _id, df in dfs])\n",
    "    dfs.to_csv(f'{save_dir}/{image_dir}/info.csv', header=False, index=False)\n",
    "    print(f'CSV saved in {save_dir}/{image_dir}/info.csv')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 実行"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !rm -rf {output_folder}\n",
    "run_all(file_name_tuples, save_dir=output_folder, image_dir=output_name, \n",
    "        dpi=dpi, output_separated_csv=output_separated_csv, **layout_info)\n",
    "!cd {output_folder}/{output_name} && zip -q -r ../{output_name}.zip *\n",
    "\n",
    "print(f'Create zip file at {os.getcwd()}/{output_folder}/{output_name}.zip')\n",
    "print(f'Please download the zip file and extract it at collection.media.' )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pdf-box-extractor-J9LyZA-i-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
